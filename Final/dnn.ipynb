{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import legacy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import legacy\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    \n",
    "    for column in ['x', 'y', 'z']:\n",
    "        df[column] = (df[column] - df[column].mean()) / df[column].std()\n",
    "    df = pd.DataFrame(df[['x', 'y', 'z']])\n",
    "    return df\n",
    "\n",
    "def segment(df, window_size, step_size):\n",
    "    segments = []\n",
    "    for start in range(0, len(df) - window_size + 1, step_size):\n",
    "        end = start + window_size\n",
    "        segment = df.iloc[start:end]\n",
    "        segments.append(segment)\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollingWindow(window_size=80, step_size=20):\n",
    "    activities_path = 'Activity'\n",
    "    activity_segments = {}\n",
    "\n",
    "    # Iterate over each activity's folder\n",
    "    for activity_name in os.listdir(activities_path):\n",
    "        print(activity_name)\n",
    "        activity_folder = os.path.join(activities_path, activity_name)\n",
    "        if os.path.isdir(activity_folder):\n",
    "            # Store segments for each activity\n",
    "            activity_segments[activity_name] = []\n",
    "            \n",
    "            # Iterate over each CSV file within the activity's folder\n",
    "            for filename in os.listdir(activity_folder):\n",
    "                if filename.endswith('.csv'):\n",
    "                    file_path = os.path.join(activity_folder, filename)\n",
    "                    \n",
    "                    # Read  CSV file\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # Normalize data\n",
    "                    df_normalized = normalize(df)\n",
    "                    \n",
    "                    # Segment the data using a rolling window\n",
    "                    segments = segment(df_normalized, window_size, step_size)\n",
    "                    \n",
    "                    # Append the segments to the activity's list\n",
    "                    activity_segments[activity_name].extend(segments)\n",
    "\n",
    "    return activity_segments\n",
    "\n",
    "def featuresAndLabels(activity_segments, len):\n",
    "    X = []  # Features\n",
    "    y = []  # Labels\n",
    "    activity_to_label = {activity: i for i, activity in enumerate(activity_segments.keys())}\n",
    "\n",
    "    for activity, segments in activity_segments.items():\n",
    "        for segment in segments:\n",
    "            if segment.shape == (len, 3):  # Adjust based off how many sec\n",
    "                X.append(segment)\n",
    "            else:\n",
    "                print(f\"Segment with shape {segment.shape} does not match the expected shape and will be skipped.\")\n",
    "            # Append the label\n",
    "            y.append(activity_to_label[activity])\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    X = np.array(X, dtype=object)  \n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def getModel(activity_segments, len):\n",
    "    # num_classes = len(activity_segments.keys())\n",
    "    # Define the model\n",
    "    model = Sequential([\n",
    "        # First LSTM layer returns sequences to feed into the next LSTM layer\n",
    "        LSTM(64, return_sequences=True, input_shape=(len, 3)),  # Adjust 'input_shape' as needed based on your data\n",
    "        # Second LSTM layer, also returning sequences\n",
    "        LSTM(64, return_sequences=True),\n",
    "        # Third LSTM layer, does not need to return sequences\n",
    "        LSTM(64),\n",
    "        # Add Dense layers for classification\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')  # 'num_classes' should be set to the number of activities you are classifying\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',  # You can adjust the optimizer, learning rate, etc.\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Display the model's architecture\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 32*5\n",
    "# fivesecactivity = rollingWindow(len, int((32*5)*.25))\n",
    "# fivesecnum_classes = len(fivesecactivity.keys())\n",
    "# fivesec_X, fivesec_y = featuresAndLabels(fivesecactivity, len)\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(fivesec_X, fivesec_y, test_size=0.15, random_state=42)\n",
    "# # Further split training+validation data into training and validation data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)\n",
    "# X_train_padded = pad_sequences(X_train, padding='post', maxlen=len) \n",
    "# X_val_padded = pad_sequences(X_val, padding='post', maxlen=len)\n",
    "# X_test_padded = pad_sequences(X_test, maxlen=len, padding='post')\n",
    "# model = getModel(fivesecactivity, 32*5)\n",
    "# history = model.fit(X_train_padded, y_train,\n",
    "#                     validation_data=(X_val_padded, y_val),\n",
    "#                     epochs=30,  # Adjust the number of epochs based on your observation of the training and validation loss/accuracy\n",
    "#                     batch_size=64)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# test_loss, test_acc = model.evaluate(X_test_padded, y_test)\n",
    "# print(f\"Test Accuracy: {test_acc}, Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motor Behavioral Task: (All conditions seen here)\n",
      "Motor Behavioral\n",
      "Training ITWT - A\n",
      "digit symbol task: (SIT)\n",
      "TWT_B training\n",
      "Training:TWT_B\n",
      "B1_T2: (WALK)\n",
      "trail making task: (SIT)\n",
      "Naughton Test: (WALK)\n",
      "B1_T1: (WALK)\n",
      "Training: TWT_A\n",
      "Motor behavioral task\n",
      "Training ITWT -B\n",
      "TNT\n",
      "TWT_A Training: (WALK)\n",
      "TM Comfortable Speed: (WALK)\n",
      "Naughton test\n",
      "B2_T1: (WALK)\n",
      "B1_TWT_A: (WALK)\n",
      "TWT_A training\n",
      "Training ITWT - B\n",
      "Training:TWT_A\n",
      "TWT_B Training: (WALK)\n",
      "Naughton\n",
      "B2_TWT_A: (WALK)\n",
      "SOT: (STAND)\n",
      "Montreal Cognitive Assessment: (SIT)\n",
      "HR Recovery: (STAND or SIT)\n",
      "B1_TWT_B: (WALK)\n",
      "B2_T2: (WALK)\n",
      "Naughton Task\n",
      "TM comfortable speed\n",
      "Training: TWT_B\n",
      "B2_TWT_B: (WALK)\n"
     ]
    }
   ],
   "source": [
    "window_size = 160  # 2.5 seconds\n",
    "step_size = 20  # 75% overlap\n",
    "\n",
    "activities_path = 'Activity'\n",
    "activity_segments = {}\n",
    "\n",
    "# Iterate over each activity's folder\n",
    "for activity_name in os.listdir(activities_path):\n",
    "    print(activity_name)\n",
    "    activity_folder = os.path.join(activities_path, activity_name)\n",
    "    if os.path.isdir(activity_folder):\n",
    "        # Store segments for each activity\n",
    "        activity_segments[activity_name] = []\n",
    "        \n",
    "        # Iterate over each CSV file within the activity's folder\n",
    "        for filename in os.listdir(activity_folder):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(activity_folder, filename)\n",
    "                \n",
    "                # Read  CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Normalize data\n",
    "                df_normalized = normalize(df)\n",
    "                \n",
    "                # Segment the data using a rolling window\n",
    "                segments = segment(df_normalized, window_size, step_size)\n",
    "                \n",
    "                # Append the segments to the activity's list\n",
    "                activity_segments[activity_name].extend(segments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(activity_segments.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# # Split training+validation data into training and validation data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)# Number of features in your input data\n",
    "# # input_shape = window_size * 3  # For X, Y, Z axes\n",
    "# # # Number of distinct activities\n",
    "# # num_classes = len(activity_segments.keys())\n",
    "\n",
    "# # model = Sequential([\n",
    "# #     Flatten(input_shape=(input_shape,)),  # Adjust 'input_shape' as necessary\n",
    "# #     Dense(128, activation='relu'),\n",
    "# #     Dropout(0.5),\n",
    "# #     Dense(64, activation='relu'),\n",
    "# #     Dense(num_classes, activation='softmax')  # 'num_classes' should match the number of activities\n",
    "# # ])\n",
    "\n",
    "# # model.compile(optimizer=legacy.Adam(learning_rate=0.001),\n",
    "# #               loss='sparse_categorical_crossentropy',\n",
    "# #               metrics=['accuracy'])\n",
    "# num_classes = len(activity_segments.keys())\n",
    "# model = Sequential([\n",
    "#     Flatten(input_shape=(80,3)),  # Updated input shape to match the data\n",
    "#     Dense(128, activation='ELU'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(64, activation='ELU'),\n",
    "#     Dense(num_classes, activation='softmax')  # Ensure num_classes matches the number of your activities\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer=legacy.Adam(learning_rate=0.001),\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# model.summary()\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                     validation_data=(X_val, y_val),\n",
    "#                     epochs=20,  # Adjust the number of epochs as needed\n",
    "#                     batch_size=64)  # And the batch size as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features = 3  # For X, Y, Z axes of accelerometer data\n",
    "# time_steps = 80  # Based on your window size\n",
    "# num_classes = len(activity_segments.keys())\n",
    "# model = Sequential([\n",
    "#     LSTM(64, input_shape=(time_steps, num_features), return_sequences=True),\n",
    "#     LSTM(32, return_sequences=False),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(num_classes, activation='softmax')  # num_classes is the number of activities\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer=legacy.Adam(learning_rate=0.001),\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []  # Features\n",
    "y = []  # Labels\n",
    "len = 96\n",
    "activity_to_label = {activity: i for i, activity in enumerate(activity_segments.keys())}\n",
    "\n",
    "for activity, segments in activity_segments.items():\n",
    "    for segment in segments:\n",
    "        # Ensure the segment is in the correct shape, e.g., (256, 3) for 256 timesteps and 3 features (X, Y, Z)\n",
    "        if segment.shape == (len, 3):  # Adjust (256, 3) based on your actual expected segment shape\n",
    "            X.append(segment)\n",
    "        else:\n",
    "            # Handle segments that don't match the expected shape, possibly with padding, trimming, or ignoring\n",
    "            print(f\"Segment with shape {segment.shape} does not match the expected shape and will be skipped.\")\n",
    "        # Append the label\n",
    "        y.append(activity_to_label[activity])\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(X, dtype=object)  \n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "# Further split training+validation data into training and validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_train_padded = pad_sequences(X_train, padding='post', maxlen=len) \n",
    "X_val_padded = pad_sequences(X_val, padding='post', maxlen=len)\n",
    "X_test_padded = pad_sequences(X_test, maxlen=len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 96, 64)            17408     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 96, 64)            33024     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 34)                2210      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89826 (350.88 KB)\n",
      "Trainable params: 89826 (350.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    # First LSTM layer returns sequences to feed into the next LSTM layer\n",
    "    LSTM(64, return_sequences=True, input_shape=(len, 3)), \n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(64),\n",
    "    # Add Dense layers for classification\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')  # 'num_classes' should be set to the number of activities you are classifying\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',  # You can adjust the optimizer, learning rate, etc.\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/29\n",
      "6590/6590 [==============================] - 956s 144ms/step - loss: 2.4706 - accuracy: 0.2009 - val_loss: 2.2277 - val_accuracy: 0.2574\n",
      "Epoch 2/29\n",
      "6590/6590 [==============================] - 893s 136ms/step - loss: 2.1276 - accuracy: 0.2797 - val_loss: 2.0484 - val_accuracy: 0.2961\n",
      "Epoch 3/29\n",
      "6590/6590 [==============================] - 889s 135ms/step - loss: 1.9797 - accuracy: 0.3181 - val_loss: 1.9419 - val_accuracy: 0.3254\n",
      "Epoch 4/29\n",
      "6590/6590 [==============================] - 924s 140ms/step - loss: 1.8859 - accuracy: 0.3444 - val_loss: 1.8581 - val_accuracy: 0.3495\n",
      "Epoch 5/29\n",
      "6590/6590 [==============================] - 930s 141ms/step - loss: 1.8172 - accuracy: 0.3641 - val_loss: 1.8053 - val_accuracy: 0.3691\n",
      "Epoch 6/29\n",
      "6590/6590 [==============================] - 917s 139ms/step - loss: 1.7621 - accuracy: 0.3797 - val_loss: 1.7617 - val_accuracy: 0.3826\n",
      "Epoch 7/29\n",
      "6590/6590 [==============================] - 929s 141ms/step - loss: 1.7172 - accuracy: 0.3921 - val_loss: 1.7368 - val_accuracy: 0.3866\n",
      "Epoch 8/29\n",
      "6590/6590 [==============================] - 928s 141ms/step - loss: 1.6773 - accuracy: 0.4048 - val_loss: 1.7037 - val_accuracy: 0.3967\n",
      "Epoch 9/29\n",
      "6590/6590 [==============================] - 885s 134ms/step - loss: 1.6448 - accuracy: 0.4141 - val_loss: 1.6778 - val_accuracy: 0.4060\n",
      "Epoch 10/29\n",
      "6590/6590 [==============================] - 887s 135ms/step - loss: 1.6152 - accuracy: 0.4237 - val_loss: 1.6515 - val_accuracy: 0.4122\n",
      "Epoch 11/29\n",
      "6590/6590 [==============================] - 917s 139ms/step - loss: 1.5889 - accuracy: 0.4309 - val_loss: 1.6395 - val_accuracy: 0.4157\n",
      "Epoch 12/29\n",
      "6590/6590 [==============================] - 920s 140ms/step - loss: 1.5663 - accuracy: 0.4377 - val_loss: 1.6154 - val_accuracy: 0.4261\n",
      "Epoch 13/29\n",
      "6590/6590 [==============================] - 893s 135ms/step - loss: 1.5451 - accuracy: 0.4436 - val_loss: 1.6181 - val_accuracy: 0.4234\n",
      "Epoch 14/29\n",
      "6590/6590 [==============================] - 923s 140ms/step - loss: 1.5270 - accuracy: 0.4493 - val_loss: 1.5908 - val_accuracy: 0.4289\n",
      "Epoch 15/29\n",
      "6590/6590 [==============================] - 885s 134ms/step - loss: 1.5093 - accuracy: 0.4549 - val_loss: 1.5812 - val_accuracy: 0.4315\n",
      "Epoch 16/29\n",
      "6590/6590 [==============================] - 885s 134ms/step - loss: 1.4941 - accuracy: 0.4596 - val_loss: 1.5715 - val_accuracy: 0.4404\n",
      "Epoch 17/29\n",
      "6590/6590 [==============================] - 883s 134ms/step - loss: 1.4801 - accuracy: 0.4640 - val_loss: 1.5643 - val_accuracy: 0.4422\n",
      "Epoch 18/29\n",
      "6590/6590 [==============================] - 869s 132ms/step - loss: 1.4689 - accuracy: 0.4681 - val_loss: 1.5487 - val_accuracy: 0.4452\n",
      "Epoch 19/29\n",
      "6590/6590 [==============================] - 873s 132ms/step - loss: 1.4578 - accuracy: 0.4711 - val_loss: 1.5563 - val_accuracy: 0.4422\n",
      "Epoch 20/29\n",
      "6590/6590 [==============================] - 880s 133ms/step - loss: 1.4450 - accuracy: 0.4747 - val_loss: 1.5423 - val_accuracy: 0.4484\n",
      "Epoch 21/29\n",
      "6590/6590 [==============================] - 877s 133ms/step - loss: 1.4419 - accuracy: 0.4759 - val_loss: 1.5347 - val_accuracy: 0.4489\n",
      "Epoch 22/29\n",
      "6590/6590 [==============================] - 875s 133ms/step - loss: 1.4288 - accuracy: 0.4805 - val_loss: 1.5343 - val_accuracy: 0.4499\n",
      "Epoch 23/29\n",
      "6590/6590 [==============================] - 857s 130ms/step - loss: 1.4192 - accuracy: 0.4825 - val_loss: 1.5228 - val_accuracy: 0.4553\n",
      "Epoch 24/29\n",
      "6590/6590 [==============================] - 859s 130ms/step - loss: 1.4116 - accuracy: 0.4857 - val_loss: 1.5217 - val_accuracy: 0.4530\n",
      "Epoch 25/29\n",
      "6590/6590 [==============================] - 855s 130ms/step - loss: 1.4050 - accuracy: 0.4874 - val_loss: 1.5149 - val_accuracy: 0.4569\n",
      "Epoch 26/29\n",
      "6590/6590 [==============================] - 844s 128ms/step - loss: 1.3983 - accuracy: 0.4899 - val_loss: 1.5104 - val_accuracy: 0.4599\n",
      "Epoch 27/29\n",
      "6590/6590 [==============================] - 884s 134ms/step - loss: 1.3911 - accuracy: 0.4919 - val_loss: 1.5066 - val_accuracy: 0.4600\n",
      "Epoch 28/29\n",
      "6590/6590 [==============================] - 881s 134ms/step - loss: 1.3884 - accuracy: 0.4931 - val_loss: 1.5094 - val_accuracy: 0.4602\n",
      "Epoch 29/29\n",
      "6590/6590 [==============================] - 885s 134ms/step - loss: 1.3794 - accuracy: 0.4954 - val_loss: 1.4952 - val_accuracy: 0.4634\n",
      "2737/2737 [==============================] - 64s 23ms/step - loss: 1.5092 - accuracy: 0.4612\n",
      "Test Accuracy: 0.4611905515193939, Test Loss: 1.5091686248779297\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded, y_train,\n",
    "                    validation_data=(X_val_padded, y_val),\n",
    "                    epochs=29,  # Adjust the number of epochs based on your observation of the training and validation loss/accuracy\n",
    "                    batch_size=64)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Accuracy: {test_acc}, Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 64 BATCH SIZE produces the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "len = 160\n",
    "# fivesecactivity = rollingWindow(len, 40)\n",
    "fivesecactivity = activity_segments\n",
    "fivesecnum_classes = num_classes ## len(fivesecactivity.keys())\n",
    "fivesec_X, fivesec_y = featuresAndLabels(fivesecactivity, len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(fivesec_X, fivesec_y, test_size=0.15, random_state=42)\n",
    "# Further split training+validation data into training and validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_padded = pad_sequences(X_train, padding='post', maxlen=len) \n",
    "X_val_padded = pad_sequences(X_val, padding='post', maxlen=len)\n",
    "X_test_padded = pad_sequences(X_test, maxlen=len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 160, 64)           17408     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 160, 64)           33024     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 34)                2210      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89826 (350.88 KB)\n",
      "Trainable params: 89826 (350.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getModel(fivesecactivity, 32*5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_padded, y_train,\n",
    "                    validation_data=(X_val_padded, y_val),\n",
    "                    epochs=30,  # Adjust the number of epochs based on your observation of the training and validation loss/accuracy\n",
    "                    batch_size=64)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Accuracy: {test_acc}, Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fast fourirer transform (fft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
